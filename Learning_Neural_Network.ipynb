{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = [[3, 4]]\n",
    "Y_data = [[1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 2])\n",
    "\n",
    "W = tf.Variable([[1., 2.], [3., 4.]], name = \"weight\")\n",
    "b = tf.Variable([1., 2.], name = \"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis), axis =1))\n",
    "optimizer =tf.train.GradientDescentOptimizer(learning_rate =0.1)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :  1 \n",
      "W:\n",
      " [[1. 2.]\n",
      " [3. 4.]] \n",
      "b: [1. 2.] \n",
      "cost 8.000336 \n",
      "hypothesis:\n",
      " [[3.000e-04 9.997e-01]] \n",
      "predicted:\n",
      " [1]\n",
      "--------------------\n",
      "step :  2 \n",
      "W:\n",
      " [[1.2999 1.7001]\n",
      " [3.3999 3.6001]] \n",
      "b: [1.1 1.9] \n",
      "cost 2.8606791 \n",
      "hypothesis:\n",
      " [[0.0572 0.9428]] \n",
      "predicted:\n",
      " [1]\n",
      "--------------------\n",
      "step :  3 \n",
      "W:\n",
      " [[1.5827 1.4173]\n",
      " [3.777  3.223 ]] \n",
      "b: [1.1942 1.8058] \n",
      "cost 0.115447275 \n",
      "hypothesis:\n",
      " [[0.891 0.109]] \n",
      "predicted:\n",
      " [0]\n",
      "--------------------\n",
      "step :  4 \n",
      "W:\n",
      " [[1.6154 1.3846]\n",
      " [3.8206 3.1794]] \n",
      "b: [1.2051 1.7949] \n",
      "cost 0.06711332 \n",
      "hypothesis:\n",
      " [[0.9351 0.0649]] \n",
      "predicted:\n",
      " [0]\n",
      "--------------------\n",
      "step :  5 \n",
      "W:\n",
      " [[1.6349 1.3651]\n",
      " [3.8466 3.1534]] \n",
      "b: [1.2116 1.7884] \n",
      "cost 0.04834303 \n",
      "hypothesis:\n",
      " [[0.9528 0.0472]] \n",
      "predicted:\n",
      " [0]\n",
      "--------------------\n",
      "step :  6 \n",
      "W:\n",
      " [[1.6491 1.3509]\n",
      " [3.8654 3.1346]] \n",
      "b: [1.2164 1.7836] \n",
      "cost 0.038020182 \n",
      "hypothesis:\n",
      " [[0.9627 0.0373]] \n",
      "predicted:\n",
      " [0]\n",
      "--------------------\n",
      "step :  7 \n",
      "W:\n",
      " [[1.6603 1.3397]\n",
      " [3.8804 3.1196]] \n",
      "b: [1.2201 1.7799] \n",
      "cost 0.03141995 \n",
      "hypothesis:\n",
      " [[0.9691 0.0309]] \n",
      "predicted:\n",
      " [0]\n",
      "--------------------\n",
      "step :  8 \n",
      "W:\n",
      " [[1.6695 1.3305]\n",
      " [3.8927 3.1073]] \n",
      "b: [1.2232 1.7768] \n",
      "cost 0.026813786 \n",
      "hypothesis:\n",
      " [[0.9735 0.0265]] \n",
      "predicted:\n",
      " [0]\n",
      "--------------------\n",
      "step :  9 \n",
      "W:\n",
      " [[1.6775 1.3225]\n",
      " [3.9033 3.0967]] \n",
      "b: [1.2258 1.7742] \n",
      "cost 0.023407187 \n",
      "hypothesis:\n",
      " [[0.9769 0.0231]] \n",
      "predicted:\n",
      " [0]\n",
      "--------------------\n",
      "step :  10 \n",
      "W:\n",
      " [[1.6844 1.3156]\n",
      " [3.9126 3.0874]] \n",
      "b: [1.2281 1.7719] \n",
      "cost 0.020781385 \n",
      "hypothesis:\n",
      " [[0.9794 0.0206]] \n",
      "predicted:\n",
      " [0]\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10):\n",
    "        W_val, b_val, cost_val, hyp_val = sess.run([W, b,cost, hypothesis], feed_dict = {X:X_data, Y:Y_data})\n",
    "        pred = sess.run(tf.argmax(hyp_val, 1))\n",
    "        print('step : ', step+1, '\\nW:\\n', np.round(W_val, 4), '\\nb:', np.round(b_val, 4), \"\\ncost\", cost_val,\n",
    "             '\\nhypothesis:\\n', np.round(hyp_val, 4),'\\npredicted:\\n',pred)\n",
    "        print('-'*20)\n",
    "        sess.run(train, feed_dict={X:X_data, Y:Y_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
